{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Importing necessary libraraies ##\n",
    "\n",
    "import re, string, unicodedata\n",
    "import nltk\n",
    "import contractions\n",
    "import inflect\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import LancasterStemmer, WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collection(Database(MongoClient(host=['localhost:27017'], document_class=dict, tz_aware=False, connect=True), 'apps'), 'app_name')\n"
     ]
    }
   ],
   "source": [
    "### Fetching data from mongo ###\n",
    "\n",
    "from pymongo import MongoClient\n",
    "client = MongoClient('localhost', 27017)\n",
    "db = client.apps\n",
    "collection = db.app_name\n",
    "print(collection)\n",
    "\n",
    "import pandas as pd\n",
    "df_apps = pd.DataFrame(list(collection.find({},{'_id':0, 'description': 1, 'category': 1, 'app_name': 1})))\n",
    "df_apps = df_apps[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Declaring preprocessing steps for text cleaning/pre-processing ###\n",
    "\n",
    "\n",
    "def remove_between_square_brackets(text):\n",
    "    \"\"\" expanding contractions like didn't haven't etc \"\"\"\n",
    "    return re.sub('\\[[^]]*\\]', '', text)\n",
    "\n",
    "def replace_contractions(text):\n",
    "    \"\"\"Replace contractions in string of text\"\"\"\n",
    "    return contractions.fix(text)\n",
    "\n",
    "def remove_non_ascii(words):\n",
    "    \"\"\"Remove non-ASCII characters from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def to_lowercase(words):\n",
    "    \"\"\"Convert all characters to lowercase from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = word.lower()\n",
    "        new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def remove_punctuation(words):\n",
    "    \"\"\"Remove punctuation from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        new_word = re.sub(r'[^\\w\\s]', '', word)\n",
    "        if new_word != '':\n",
    "            new_words.append(new_word)\n",
    "    return new_words\n",
    "\n",
    "def replace_numbers(words):\n",
    "    \"\"\"Replace all interger occurrences in list of tokenized words with textual representation\"\"\"\n",
    "    p = inflect.engine()\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word.isdigit():\n",
    "            new_word = p.number_to_words(word)\n",
    "            new_words.append(new_word)\n",
    "        else:\n",
    "            new_words.append(word)\n",
    "    return new_words\n",
    "\n",
    "def remove_stopwords(words):\n",
    "    \"\"\"Remove stop words from list of tokenized words\"\"\"\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        if word not in stopwords.words('english'):\n",
    "            new_words.append(word)\n",
    "    return new_words\n",
    "\n",
    "def stem_words(words):\n",
    "    \"\"\"Stem words in list of tokenized words\"\"\"\n",
    "    stemmer = LancasterStemmer()\n",
    "    stems = []\n",
    "    for word in words:\n",
    "        stem = stemmer.stem(word)\n",
    "        stems.append(stem)\n",
    "    return stems\n",
    "\n",
    "def lemmatize_verbs(words):\n",
    "    \"\"\"Lemmatize verbs in list of tokenized words\"\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmas = []\n",
    "    for word in words:\n",
    "        lemma = lemmatizer.lemmatize(word, pos='v')\n",
    "        lemmas.append(lemma)\n",
    "    return lemmas\n",
    "\n",
    "def normalize(words):\n",
    "    words = remove_non_ascii(words)\n",
    "    words = to_lowercase(words)\n",
    "    words = remove_punctuation(words)\n",
    "    words = replace_numbers(words)\n",
    "    words = remove_stopwords(words)\n",
    "    return words\n",
    "\n",
    "\n",
    "def stem_and_lemmatize(words):\n",
    "    stems = stem_words(words)\n",
    "    lemmas = lemmatize_verbs(words)\n",
    "    return stems, lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/gaurav/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Download wordnet if not exists ##\n",
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dataprep_full(row):\n",
    "    try:\n",
    "        ### Applying cleaning methods ###\n",
    "        val = replace_contractions(row['description'])\n",
    "        val = remove_between_square_brackets(val)\n",
    "        val = nltk.word_tokenize(val)\n",
    "        val = remove_non_ascii(val)\n",
    "        val = to_lowercase(val)\n",
    "        val = remove_punctuation(val)\n",
    "        val = replace_numbers(val)\n",
    "        val = remove_stopwords(val)\n",
    "        val = stem_words(val)\n",
    "        val = lemmatize_verbs(val)\n",
    "        #val = normalize(val)\n",
    "        val = ' '.join(val)\n",
    "        return val\n",
    "    except:\n",
    "        return 'NA'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Importing TF-IDF vectorization libraries ##\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Getting values for source app ##\n",
    "\n",
    "test_df = pd.DataFrame(list(collection.find({'app_name': 'Flipkart'},{'_id':0, 'description': 1, 'category': 1, 'app_name': 1})))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "979\n",
      "980\n",
      "980\n",
      "980\n"
     ]
    }
   ],
   "source": [
    "### Performing cleaning and data preparation on target and train dataframe ###\n",
    "\n",
    "df_apps.dropna(subset=['description'], inplace = True)\n",
    "test_set = test_df.apply(dataprep_full, axis = 1)\n",
    "train_set = df_apps.apply(dataprep_full, axis = 1)\n",
    "\n",
    "\n",
    "test_set_2 = test_df['description']\n",
    "train_set_2 = df_apps['description']\n",
    "combined_df_series_2 = pd.concat([test_set_2, train_set_2])\n",
    "combined_df_series = pd.concat([test_set, train_set])\n",
    "combined_app_names = pd.concat([test_df['app_name'], df_apps['app_name']])\n",
    "\n",
    "\n",
    "print(len(combined_app_names))\n",
    "print(len(combined_df_series))\n",
    "print(len(combined_df_series_2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x126910668>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create histogram for lenght of the data #\n",
    "df_apps[\"Length\"]= df_apps[\"description\"].str.len() \n",
    "ax = df_apps['Length'].plot.hist(bins=5, alpha=0.5)\n",
    "ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix_train = tfidf_vectorizer.fit_transform(combined_df_series)  #finds the tfidf score with normalization\n",
    "result = cosine_similarity(tfidf_matrix_train[0:1], tfidf_matrix_train) #here the first element of tfidf_matrix_train is matched with other three elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Create histogram of lengths to show more analysis for documents\n",
    "## Pass in only value as name and return name with scores\n",
    "\n",
    "## Find ways to do this for just one message, instead of all the messages all the time\n",
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>app_name</th>\n",
       "      <th>cosine_dist</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Flipkart</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>430</td>\n",
       "      <td>Flipkart</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>431</td>\n",
       "      <td>Snapdeal</td>\n",
       "      <td>0.364299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>434</td>\n",
       "      <td>HomeShop18</td>\n",
       "      <td>0.351901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>447</td>\n",
       "      <td>DHgate Mobile online wholesale</td>\n",
       "      <td>0.297361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>449</td>\n",
       "      <td>DHgate Mobile</td>\n",
       "      <td>0.297361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>438</td>\n",
       "      <td>Wanelo Shopping</td>\n",
       "      <td>0.280756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>429</td>\n",
       "      <td>eBay</td>\n",
       "      <td>0.273683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>464</td>\n",
       "      <td>Amazon</td>\n",
       "      <td>0.269543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>443</td>\n",
       "      <td>AliExpress Super Deals</td>\n",
       "      <td>0.260423</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           app_name  cosine_dist\n",
       "0                          Flipkart     1.000000\n",
       "430                        Flipkart     1.000000\n",
       "431                        Snapdeal     0.364299\n",
       "434                      HomeShop18     0.351901\n",
       "447  DHgate Mobile online wholesale     0.297361\n",
       "449                   DHgate Mobile     0.297361\n",
       "438                 Wanelo Shopping     0.280756\n",
       "429                            eBay     0.273683\n",
       "464                          Amazon     0.269543\n",
       "443          AliExpress Super Deals     0.260423"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df = pd.DataFrame({'app_name': combined_app_names, 'cosine_dist': result[0]}).sort_values(['cosine_dist'], ascending=False)\n",
    "result_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>app_name</th>\n",
       "      <th>cosine_dist</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Flipkart</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>430</td>\n",
       "      <td>Flipkart</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>431</td>\n",
       "      <td>Snapdeal</td>\n",
       "      <td>0.337076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>434</td>\n",
       "      <td>HomeShop18</td>\n",
       "      <td>0.314960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>464</td>\n",
       "      <td>Amazon</td>\n",
       "      <td>0.281992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>447</td>\n",
       "      <td>DHgate Mobile online wholesale</td>\n",
       "      <td>0.268639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>449</td>\n",
       "      <td>DHgate Mobile</td>\n",
       "      <td>0.268639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>438</td>\n",
       "      <td>Wanelo Shopping</td>\n",
       "      <td>0.265911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>433</td>\n",
       "      <td>Jabong</td>\n",
       "      <td>0.256929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>429</td>\n",
       "      <td>eBay</td>\n",
       "      <td>0.229808</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           app_name  cosine_dist\n",
       "0                          Flipkart     1.000000\n",
       "430                        Flipkart     1.000000\n",
       "431                        Snapdeal     0.337076\n",
       "434                      HomeShop18     0.314960\n",
       "464                          Amazon     0.281992\n",
       "447  DHgate Mobile online wholesale     0.268639\n",
       "449                   DHgate Mobile     0.268639\n",
       "438                 Wanelo Shopping     0.265911\n",
       "433                          Jabong     0.256929\n",
       "429                            eBay     0.229808"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Direct approach without vectorization ###\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix_train = tfidf_vectorizer.fit_transform(combined_df_series_2)  #finds the tfidf score with normalization\n",
    "result = cosine_similarity(tfidf_matrix_train[0:1], tfidf_matrix_train) #here the first element of tfidf_matrix_train is matched with other three elements\n",
    "\n",
    "result_df_2 = pd.DataFrame({'app_name': combined_app_names, 'cosine_dist': result[0]}).sort_values(['cosine_dist'], ascending=False)\n",
    "result_df_2.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "similar_apps",
   "language": "python",
   "name": "similarapps"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
